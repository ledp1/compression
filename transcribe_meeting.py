#!/usr/bin/env python3
"""
Transcribe audio from a meeting video to a timestamped .txt file.
Uses moviepy to extract audio and openai-whisper for transcription.
"""

import sys
from pathlib import Path
from typing import List

# Source video (path from project's compress_video.py)
VIDEO_PATH = Path(
    "/Users/myhomefolder/Downloads/Hardware_AWG/Meeting materials/2025-08-28/"
    "Hardware AWG meeting - 2025_08_28 07_50 PDT - Recording.mp4"
)
PROJECT_DIR = Path(__file__).resolve().parent
OUTPUT_TXT = PROJECT_DIR / "meeting_transcript.txt"
TEMP_AUDIO = PROJECT_DIR / "_temp_meeting_audio.wav"

# Whisper model: "tiny", "base", "small", "medium", "large" (larger = better quality, slower)
WHISPER_MODEL = "base"


def sec_to_timestamp(seconds: float) -> str:
    """Convert seconds to HH:MM:SS.mmm."""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:06.3f}"


def extract_audio_moviepy(video_path: Path, audio_path: Path) -> None:
    """Extract audio from video to WAV using moviepy."""
    from moviepy import VideoFileClip

    print("Extracting audio with moviepy...")
    clip = VideoFileClip(str(video_path))
    clip.audio.write_audiofile(str(audio_path))
    clip.close()


def transcribe_with_whisper(audio_path: Path) -> List[dict]:
    """Transcribe audio with openai-whisper; return list of segments with start, end, text."""
    import whisper

    print(f"Loading Whisper model '{WHISPER_MODEL}'...")
    model = whisper.load_model(WHISPER_MODEL)
    print("Transcribing...")
    result = model.transcribe(str(audio_path), fp16=False)
    return result.get("segments", [])


def write_timestamped_transcript(segments: List[dict], out_path: Path) -> None:
    """Write transcript with timestamps to a text file."""
    lines = [
        "Meeting transcript (with timestamps)",
        "Source: " + str(VIDEO_PATH),
        "Generated by transcribe_meeting.py",
        "",
        "---",
        "",
    ]
    for seg in segments:
        start = seg.get("start", 0)
        end = seg.get("end", 0)
        text = (seg.get("text") or "").strip()
        if not text:
            continue
        ts_range = f"[{sec_to_timestamp(start)} - {sec_to_timestamp(end)}]"
        lines.append(f"{ts_range} {text}")
        lines.append("")
    out_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"Transcript written to {out_path}")


def main() -> int:
    if not VIDEO_PATH.exists():
        print(f"Video not found: {VIDEO_PATH}", file=sys.stderr)
        return 1

    PROJECT_DIR.mkdir(parents=True, exist_ok=True)

    try:
        extract_audio_moviepy(VIDEO_PATH, TEMP_AUDIO)
        segments = transcribe_with_whisper(TEMP_AUDIO)
        write_timestamped_transcript(segments, OUTPUT_TXT)
        return 0
    finally:
        if TEMP_AUDIO.exists():
            TEMP_AUDIO.unlink()
            print("Removed temporary audio file.")


if __name__ == "__main__":
    sys.exit(main())
